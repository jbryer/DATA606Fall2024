[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Announcements",
    "section": "",
    "text": "Be sure to check this page regularly for updates. In addition to periodic updates about the course, slides and recordings of the weekly meetups will be posted here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to the Course\n\n\n\n\n\nSlides and recording for the Intro to the Course meetup.\n\n\n\n\n\nAug 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to DATA606!\n\n\n\n\n\nImportant information on how to get started with this course. Please read this post carefully.\n\n\n\n\n\nJun 29, 2024\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Announcements"
    ]
  },
  {
    "objectID": "chapters/chapter8.html",
    "href": "chapters/chapter8.html",
    "title": "Chapter 8 - Linear Regression",
    "section": "",
    "text": "Learning Outcomes\n\nDefine the explanatory variable as the independent variable (predictor), and the response variable as the dependent variable (predicted).\nPlot the explanatory variable (\\(x\\)) on the x-axis and the response variable (\\(y\\)) on the y-axis, and fit a linear regression model $y = \\beta_0 + \\beta_1 x$ where \\(\\beta_0\\) is the intercept, and \\(\\beta_1\\) is the slope.\n\nNote that the point estimates (estimated from observed data) for \\(\\beta_0\\) and \\(\\beta_1\\) are \\(b_0\\) and \\(b_1\\), respectively.\n\nWhen describing the association between two numerical variables, evaluate\n\ndirection: positive (\\(x \\uparrow, y \\uparrow\\)), negative (\\(x \\downarrow, y \\uparrow\\))\nform: linear or not\nstrength: determined by the scatter around the underlying relationship\n\nDefine correlation as the association between two numerical variables.\n\nNote that a relationship that is nonlinear is simply called an association.\n\nNote that correlation coefficient (\\(r\\), also called Pearson’s \\(r\\)) the following properties:\n\nthe magnitude (absolute value) of the correlation coefficient measures the strength of the linear association between two numerical variables\nthe sign of the correlation coefficient indicates the direction of association\nthe correlation coefficient is always between -1 and 1, inclusive, with -1 indicating perfect negative linear association, +1 indicating perfect positive linear association, and 0 indicating no relationship\nthe correlation coefficient is unitless\nsince the correlation coefficient is unitless, it is not affected by changes in the center or scale of either variable (such as unit conversions)\nthe correlation of X with Y is the same as of Y with X\nthe correlation coefficient is sensitive to outliers\n\nRecall that correlation does not imply causation.\nDefine residual (\\(e\\)) as the difference between the observed (\\(y\\)) and predicted (\\(\\hat{y}\\)) values of the response variable. $e_i = y_i - \\hat{y}_i$\nDefine the least squares line as the line that minimizes the sum of the squared residuals, and list conditions necessary for fitting such line:\n\nlinearity\nnearly normal residuals\nconstant variability\n\nDefine an indicator variable as a binary explanatory variable (with two levels).\nCalculate the estimate for the slope (\\(b_1\\)) as $b_1 = R\\frac{s_y}{s_x}$, where \\(r\\) is the correlation coefficient, \\(s_y\\) is the standard deviation of the response variable, and \\(s_x\\) is the standard deviation of the explanatory variable.\nInterpret the slope as\n\n“For each unit increase in \\(x\\), we would expect \\(y\\) to increase/decrease on average by \\(|b_1|\\) units” when \\(x\\) is numerical.\n“The average increase/decrease in the response variable when between the baseline level and the other level of the explanatory variable is \\(|b_1|\\).” when \\(x\\) is categorical.\nNote that whether the response variable increases or decreases is determined by the sign of \\(b_1\\).\n\nNote that the least squares line always passes through the average of the response and explanatory variables (\\(\\bar{x},\\bar{y}\\)).\nUse the above property to calculate the estimate for the slope (\\(b_0\\)) as $b_0 = \\bar{y} - b_1 \\bar{x}$, where \\(b_1\\) is the slope, \\(\\bar{y}\\) is the average of the response variable, and \\(\\bar{x}\\) is the average of explanatory variable.\nInterpret the intercept as\n\n“When \\(x = 0\\), we would expect \\(y\\) to equal, on average, \\(b_0\\).” when \\(x\\) is numerical.\n“The expected average value of the response variable for the reference level of the explanatory variable is \\(b_0\\).” when \\(x\\) is categorical.\n\nPredict the value of the response variable for a given value of the explanatory variable, \\(x^\\star\\), by plugging in \\(x^\\star\\) in the in the linear model: $\\hat{y} = b_0 + b_1 x^\\star$\n\nOnly predict for values of \\(x^\\star\\) that are in the range of the observed data.\nDo not extrapolate beyond the range of the data, unless you are confident that the linear pattern continues.\n\nDefine \\(R^2\\) as the percentage of the variability in the response variable explained by the the explanatory variable.\n\nFor a good model, we would like this number to be as close to 100% as possible.\nThis value is calculated as the square of the correlation coefficient, and is between 0 and 1, inclusive.\n\nDefine a leverage point as a point that lies away from the center of the data in the horizontal direction.\nDefine an influential point as a point that influences (changes) the slope of the regression line.\n\nThis is usually a leverage point that is away from the trajectory of the rest of the data.\n\nDo not remove outliers from an analysis without good reason.\nBe cautious about using a categorical explanatory variable when one of the levels has very few observations, as these may act as influential points.\nDetermine whether an explanatory variable is a significant predictor for the response variable using the \\(t\\)-test and the associated p-value in the regression output.\nSet the null hypothesis testing for the significance of the predictor as $H_0: \\beta_1 = 0$, and recognize that the standard software output yields the p-value for the two-sided alternative hypothesis.\n\nNote that \\(\\beta_1 = 0\\) means the regression line is horizontal, hence suggesting that there is no relationship between the explanatory and the response variables.\n\nCalculate the T score for the hypothesis test as $T_{df}=\\frac { b_{ 1 }-{ null\\quad value } }{ SE_{ b_{ 1 } } }$ with \\(df = n - 2\\).\n\nNote that the T score has \\(n - 2\\) degrees of freedom since we lose one degree of freedom for each parameter we estimate, and in this case we estimate the intercept and the slope.\n\nNote that a hypothesis test for the intercept is often irrelevant since it’s usually out of the range of the data, and hence it is usually an extrapolation.\nCalculate a confidence interval for the slope as $b_1 \\pm t^\\star_{df} SE_{b_1}$ where \\(df = n - 2\\) and $t^\\star_{df}$ is the critical score associated with the given confidence level at the desired degrees of freedom.\n\nNote that the standard error of the slope estimate $SE_{b_1}$ can be found on the regression output.\n\n\n\n\nSupplemental Readings\n\nOpenIntro Statistics slides\nInteraction Terms\nRegerssion for non-linear terms\nLinear regression with SAT scores - This document outlines the implementation of linear regression step-by-step emphasizing visualizations.\n\n\n\nVideos",
    "crumbs": [
      "Topics",
      "8 - Linear Regression"
    ]
  },
  {
    "objectID": "chapters/chapter7.html",
    "href": "chapters/chapter7.html",
    "title": "Chapter 7 - Inference for Numerical Data",
    "section": "",
    "text": "Learning Outcomes\n\nUse the \\(t\\)-distribution for inference on a single mean, difference of paired (dependent) means, and difference of independent means.\nExplain why the \\(t\\)-distribution helps make up for the additional variability introduced by using \\(s\\) (sample standard deviation) in calculation of the standard error, in place of \\(\\sigma\\) (population standard deviation).\nDescribe how the \\(t\\)-distribution is different from the normal distribution, and what ?heavy tail? means in this context.\nNote that the \\(t\\)-distribution has a single parameter, degrees of freedom, and as the degrees of freedom increases this distribution approaches the normal distribution.\nUse a \\(t\\)-statistic, with degrees of freedom \\(df = n - 1\\) for inference for a population mean:\n\nStandard error: \\(SE = \\frac{s}{\\sqrt{n}}\\)\nConfidence interval: $\\bar{x} \\pm t_{df}^\\star SE$\nHypothesis test: $T_{df} = \\frac{\\bar{x} - \\mu}{SE}$ \\end{itemize}\n\nDescribe how to obtain a p-value for a \\(t\\)-test and a critical \\(t\\)-score ($t^\\star_{df}$) for a confidence interval.\nDefine observations as paired if each observation in one dataset has a special correspondence or connection with exactly one observation in the other data set.\nCarry out inference for paired data by first subtracting the paired observations from each other, and then treating the set of differences as a new numerical variable on which to do inference (such as a confidence interval or hypothesis test for the average difference).\nCalculate the standard error of the difference between means of two paired (dependent) samples as $SE = \\frac{s_{diff}}{\\sqrt{n_{diff}}}$ and use this standard error in hypothesis testing and confidence intervals comparing means of paired (dependent) groups.\nUse a \\(t\\)-statistic, with degrees of freedom $df = n_{diff} - 1$ for inference for a population mean: \\begin{itemize}\n\nStandard error: \\(SE = \\frac{s}{\\sqrt{n}}\\)\nConfidence interval: $\\bar{x}_{diff} \\pm t_{df}^\\star SE$\nHypothesis test: $T_{df} = \\frac{\\bar{x}_{diff} - \\mu_{diff}}{SE}$. Note that $\\mu_{diff}$ is often 0, since often $H_0: \\mu_{diff} = 0$.\n\nRecognize that a good interpretation of a confidence interval for the difference between two parameters includes a comparative statement (mentioning which group has the larger parameter).\nRecognize that a confidence interval for the difference between two parameters that doesn?t include 0 is in agreement with a hypothesis test where the null hypothesis that sets the two parameters equal to each other is rejected.\nCalculate the standard error of the difference between means of two independent samples as $SE = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}$, and use this standard error in hypothesis testing and confidence intervals comparing means of independent groups.\nUse a \\(t\\)-statistic, with degrees of freedom $df = min(n_1 - 1, n_2 - 1)$ for inference for a population mean:\n\nStandard error: $\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}$\nConfidence interval: $(\\bar{x}_1 - \\bar{x}_2) \\pm t_{df}^\\star SE$\nHypothesis test: $T_{df} = \\frac{(\\bar{x}_1 - \\bar{x}_2) - (\\mu_1 - \\mu_2)}{SE}$. Note that \\(\\mu_{diff}\\) is often 0, since often $H_0: \\mu_1 - \\mu_2 = 0$.\n\nCalculate the power of a test for a given effect size and significance level in two steps: (1) Find the cutoff for the sample statistic that will allow the null hypothesis to be rejected at the given significance level, (2) Calculate the probability of obtaining that sample statistic given the effect size.\nExplain how power changes for changes in effect size, sample size, significance level, and standard error.\nDefine analysis of variance (ANOVA) as a statistical inference method that is used to determine if the variability in the sample means is so large that it seems unlikely to be from chance alone by simultaneously considering many groups at once.\nRecognize that the null hypothesis in ANOVA sets all means equal to each other, and the alternative hypothesis suggest that at least one mean is different. \\begin{itemize}\n\n$H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_k$\n\\(H_A:\\) At least one mean is different\n\nList the conditions necessary for performing ANOVA\n\nthe observations should be independent within and across groups\nthe data within each group are nearly normal\nthe variability across the groups is about equal\nand check if they are met using graphical diagnostics.\n\nRecognize that the test statistic for ANOVA, the F statistic, is calculated as the ratio of the mean square between groups (MSG, variability between groups) and mean square error (MSE, variability within errors), and has two degrees of freedom, one for the numerator ($df_{G} = k - 1$, where \\(k\\) is the number of groups) and one for the denominator ($df_{E} = n - k$, where \\(n\\) is the total sample size).\n\nNote that you won’t be expected to calculate MSG or MSE from the raw data, but you should have a conceptual understanding of how they’re calculated and what they measure.\n\nDescribe why calculation of the p-value for ANOVA is always “one sided”.\nDescribe why conducting many \\(t\\)-tests for differences between each pair of means leads to an increased Type 1 Error rate, and we use a corrected significance level (Bonferroni corection, \\(\\alpha^\\star = \\alpha / K\\), where \\(K\\) is the e number of comparisons being considered) to combat inflating this error rate.\nDescribe why it is possible to reject the null hypothesis in ANOVA but not find significant differences between groups as a result of pairwise comparisons.\n\n\n\nSupplemental Readings\n\nOpenIntro Statistics slides\n\n\n\nVideos",
    "crumbs": [
      "Topics",
      "7 - Inference for Numerical Data"
    ]
  },
  {
    "objectID": "chapters/chapter4.html",
    "href": "chapters/chapter4.html",
    "title": "Chapter 4 - Distributions of Random Variables",
    "section": "",
    "text": "Learning Outcomes\n\nDefine the standardized (Z) score of a data point as the number of standard deviations it is away from the mean: \\(Z = \\frac{x - \\mu}{\\sigma}\\).\nUse the Z score\n\nif the distribution is normal: to determine the percentile score of a data point (using technology or normal probability tables)\nregardless of the shape of the distribution: to assess whether or not the particular observation is considered to be unusual (more than 2 standard deviations away from the mean)\n\nDepending on the shape of the distribution determine whether the median would have a negative, positive, or 0 Z score.\nAssess whether or not a distribution is nearly normal using the 68-95-99.7% rule or graphical methods such as a normal probability plot.\n\nReading: Section 4.1 of OpenIntro Statistics\nTest yourself: True/False: In a right skewed distribution the Z score of the median is positive.\n\nIf X is a random variable that takes the value 1 with probability of success \\(p\\) and 0 with probability of success \\(1-p\\), then \\(X\\) is a Bernoulli random variable.\nThe geometric distribution is used to describe how many trials it takes to observe a success.\nDefine the probability of finding the first success in the \\(n^{th}\\) trial as \\((1-p)^{n-1}p\\).\n\n\\(\\mu = \\frac{1}{p}\\)\n\\(\\sigma^2 = \\frac{1-p}{p^2}\\)\n\\(\\sigma = \\sqrt{\\frac{1-p}{p^2}}\\)\n\nDetermine if a random variable is binomial using the four conditions:\n\nThe trials are independent.\nThe number of trials, n, is fixed.\nEach trial outcome can be classified as a success or failure.\nThe probability of a success, p, is the same for each trial.\n\nCalculate the number of possible scenarios for obtaining \\(k\\) successes in \\(n\\) trials using the choose function: \\({n \\choose k} = \\frac{n!}{k!~(n - k)!}\\).\nCalculate probability of a given number of successes in a given number of trials using the binomial distribution: \\(P(k = K) = \\frac{n!}{k!~(n - k)!}~p^k~(1-p)^{(n - k)}\\).\nCalculate the expected number of successes in a given number of binomial trials \\((\\mu = np)\\) and its standard deviation \\((\\sigma = \\sqrt{np(1-p)})\\).\nWhen number of trials is sufficiently large (\\(np \\ge 10\\) and \\(n(1-p) \\ge 10\\)), use normal approximation to calculate binomial probabilities, and explain why this approach works.\n\n\n\nSupplemental Readings\n\nOpenIntro Statistics slides\n\n\n\nVideos",
    "crumbs": [
      "Topics",
      "4 - Distributions"
    ]
  },
  {
    "objectID": "chapters/chapter1.html",
    "href": "chapters/chapter1.html",
    "title": "Chapter 1 - Introduction to Data",
    "section": "",
    "text": "Learning Objectives\n\nIdentify the type of variables (e.g. numerical or categorical; discrete or continuous; ordered or not ordered).\nIdentify the relationship between multiple variables (i.e. independent vs. dependent).\nDefine variables that are not associated as independent.\nBe able to describe and identify the difference between observational and experimental studies.\nDistinguish between simple random, stratified, and cluster sampling, and recognize the benefits and drawbacks of choosing one sampling scheme over another.\nIdentify the four principles of experimental design and recognize their purposes: control any possible con- founders, randomize into treatment and control groups, replicate by using a sufficiently large sample or repeating the experiment, and block any variables that might influence the response.\n\n\n\nSupplemental Readings\n\nOpenIntro Statistics slides\n\n\n\nVideos\nOpenIntro provides a number of videos. You may find these helpful while reading the chapter.\nCase Study: Using Stents to Prevent Strokes\n\n\nData Basics: Observations, Variable, and Data Matrices\n\n\nData Collection Principles\n\n\nObservational Studies and Sampling Strategies\n\n\nDesigning Experiments\n\n\nUsing Randomization to Analyze a Gender Discrimination Study",
    "crumbs": [
      "Topics",
      "1 - Intro to Data"
    ]
  },
  {
    "objectID": "chapters/chapter3.html",
    "href": "chapters/chapter3.html",
    "title": "Chapter 3 - Probability",
    "section": "",
    "text": "Learning Outcomes\n\nDefine trial, outcome, and sample space.\nDefine and describe the law of large numbers.\nDistinguish disjoint (also called mutually exclusive) and independent events.\nUse Venn diagrams to represent events and their probabilities.\nDescribe probability distributions.\nDistinguish between marginal and conditional probabilities.\nUse tree diagrams and/or Bayes Theorem to calculate conditional probabilities and probabilities of intersection of non-independent events.\nThe expected value of a discrete random variable is computed by adding each outcome weighted by its probability.\n$$ E(X)=\\mu=\\sum_{i=1}^{k}{{x}_{i}P\\left(X={x}_{i}\\right)} $$\nThe variance of a discrete random variable is computed by adding each squared deviation of an outcome from the expected value weighted by its probability. The standard deviation is the square root of the variance.\n$$ Var(X)={\\sigma}^{2}=\\sum_{i=1}^{k}{{\\left({x}_{i}-\\mu\\right)}P\\left(X={x}_{i}\\right) } $$\nThe average of a linear combination of discrete random variables is computed as the sum of their averages, weighted by the constant multipliers.\nThe variance of a linear combination of independent discrete random variables is computed as the sum of their variances, weighted by the square of the constant multipliers.\nThe distribution of a continuous random variable is described by the probability density function.\nThe total area under the density curve is 1.\nProbabilities under the density curve can be calculated as the area under the curve.\nThe probability of a continuous random variable being exactly equal to a value is 0, since there is no area under the curve at a given location.\n\n\n\nSupplemental Readings\n\nOpenIntro Statistics slides\nMatloff, N. (2009). From Algorithms to Z-Scores: Probabilistic and Statistical Modeling in Computer Science. Available from http://heather.cs.ucdavis.edu/probstatbook.\n\n\n\nVideos\nProbability Introduction\n\n\nWould You Take This Bet?\n\n\nThe Monty Hall Problem",
    "crumbs": [
      "Topics",
      "3 - Probability"
    ]
  },
  {
    "objectID": "posts/2024-08-28-Intro_to_the_Course.html",
    "href": "posts/2024-08-28-Intro_to_the_Course.html",
    "title": "Intro to the Course",
    "section": "",
    "text": "Click here to open the slides (PDF)."
  },
  {
    "objectID": "assignments/participation.html",
    "href": "assignments/participation.html",
    "title": "Participation",
    "section": "",
    "text": "One Minute Papers\nA “one minute paper” (Angelo & Cross, 1993) is a short written reflection to be completed after each class meetup. You are to answer two questions: 1) What was the most important thing you learned during this class? and 2) What important question remains unanswered for you? Our goal is to give you a moment to reflect on the most important concepts presented were and to provide me with information about what concepts are still unclear. At the completion of each meetup (whether attended live or after watching the recording), complete the Google Form linked from the last slide.\n\n\nSlack\nPlease be an active participant in the Slack channel. In addition to being a good resource for asking and answer questions, we hope you begin to make connections with other students. Contribute at least one resource related to data science that you find and think would be useful to your fellow students. Please post it in the #resource channel.",
    "crumbs": [
      "Assignments",
      "Participation"
    ]
  },
  {
    "objectID": "assignments/daacs.html",
    "href": "assignments/daacs.html",
    "title": "DAACS",
    "section": "",
    "text": "Formative assessments are a type of assessment that help students and teachers monitor student learning. These are low stakes assessments (i.e. you get credit for doing them, not based upon how you did). I will use the results to inform my instruction.\n\n\nComplete this Google Form. The purpose of this survey is for us to get to know you better. There is no passing or failing, right or wrong answers. Please answer each section honestly. We will use the results in aggregate to inform the instruction in this class.\n\n\n\n\nThe Diagnostic Assessment and Achievement of College Skills (DAACS) is a formative assessment designed to provide you with information about key college skills. DAACS includes assessments in self-regulated learning, mathematics, reading, and writing. YOU ARE ONLY REQUIRED TO COMPLETE THE SELF-REGULATED LEARNING ASSESSMENT. This should take about 10 minutes to complete the assessment and there is no passing or failing. Once you are done, we encourage you to review the resources recommended to you. We will use the aggregated results in class. To get credit for this assignment:\n\nGo to cuny.daacs.net\nLogin using your CUNY account.\nComplete the Self-Regulated Learning (SRL) assessment.\n\nDownload the PDF once complete and upload it to Brightspace.",
    "crumbs": [
      "Assignments",
      "DAACS"
    ]
  },
  {
    "objectID": "assignments/daacs.html#formative-assessments",
    "href": "assignments/daacs.html#formative-assessments",
    "title": "DAACS",
    "section": "",
    "text": "Formative assessments are a type of assessment that help students and teachers monitor student learning. These are low stakes assessments (i.e. you get credit for doing them, not based upon how you did). I will use the results to inform my instruction.\n\n\nComplete this Google Form. The purpose of this survey is for us to get to know you better. There is no passing or failing, right or wrong answers. Please answer each section honestly. We will use the results in aggregate to inform the instruction in this class.\n\n\n\n\nThe Diagnostic Assessment and Achievement of College Skills (DAACS) is a formative assessment designed to provide you with information about key college skills. DAACS includes assessments in self-regulated learning, mathematics, reading, and writing. YOU ARE ONLY REQUIRED TO COMPLETE THE SELF-REGULATED LEARNING ASSESSMENT. This should take about 10 minutes to complete the assessment and there is no passing or failing. Once you are done, we encourage you to review the resources recommended to you. We will use the aggregated results in class. To get credit for this assignment:\n\nGo to cuny.daacs.net\nLogin using your CUNY account.\nComplete the Self-Regulated Learning (SRL) assessment.\n\nDownload the PDF once complete and upload it to Brightspace.",
    "crumbs": [
      "Assignments",
      "DAACS"
    ]
  },
  {
    "objectID": "assignments/exams.html",
    "href": "assignments/exams.html",
    "title": "Exams",
    "section": "",
    "text": "There will be a midterm and final exam. They will be posted on Blackboard during the time indicated on the course schedule.",
    "crumbs": [
      "Assignments",
      "Exams"
    ]
  },
  {
    "objectID": "assignments/homework.html",
    "href": "assignments/homework.html",
    "title": "Homework",
    "section": "",
    "text": "The solutions to the practice problems are at the end of the book and do not need to be handed in. Graded assignments should use the provided R markdown templates provided below. Data for the homework assignments, and for within the chapters too, can be downloaded here. Or alternatively all the data is included in the openintro R packge (use the data(package = 'openintro') command to list all the datasets available in that package). Right click on the “Template” link and choose “Save link as…” to save the R markdown file to your computer. By default, the Rmarkdown files should generate PDFs. This is the preferred format since PDFs can be uploaded to Blackboard. See the software course page for instructions on installing LaTeX.\n\nChapter 1 - Introduction to Data (Template)\nChapter 2 - Summarizing Data (Template)\nChapter 3 - Probability (Template)\nChapter 4 - Distributions of Random Variables (Template)\nChapter 5 - Foundations for Inference (Template)\nChapter 6 - Inference for Categorical Data (Template)\nChapter 7 - Inference for Numerical Data (Template)\nChapter 8 - Introduction to Linear Regression (Template)\nChapter 9 - Multiple and Logistic Regression (Template)",
    "crumbs": [
      "Assignments",
      "Homework"
    ]
  },
  {
    "objectID": "course/schedule.html",
    "href": "course/schedule.html",
    "title": "DATA606 - Statistics and Probability",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\nClick here to join the meetups.\n\n\n\n\n\n\n\n\n\n\nDate\nStartTime\nTopic\nSlides\nVideo\n\n\n\n\nWed, Aug 28\n8:00 pm\nIntro to the Course\n    \n\n\n\nWed, Sep 04\n8:00 pm\nIntro to Data\n\n\n\n\nWed, Sep 11\n8:00 pm\nSummarizing Data Part 1\n\n\n\n\nWed, Sep 18\n8:00 pm\nSummarizing Data Part 2\n\n\n\n\nWed, Sep 25\n8:00 pm\nProbability\n\n\n\n\nWed, Oct 02\n8:00 pm\nDistributions\n\n\n\n\nWed, Oct 09\n8:00 pm\nFoundation for Inference\n\n\n\n\nWed, Oct 16\n8:00 pm\nInference for Categorical Data\n\n\n\n\nWed, Oct 23\n8:00 pm\nInference for Numerical Data\n\n\n\n\nWed, Oct 30\n8:00 pm\nLinear Regression\n\n\n\n\nWed, Nov 06\n8:00 pm\nMaximum Likelihood Estimation\n\n\n\n\nWed, Nov 13\n8:00 pm\nMultiple Regression\n\n\n\n\nWed, Nov 20\n8:00 pm\nPredictive Modeling\n\n\n\n\nWed, Nov 27\n\nNO MEETUP - Thanksgiving\n\n\n\n\nWed, Dec 04\n8:00 pm\nBayesian Analysis\n\n\n\n\nWed, Dec 11\n8:00 pm\nFinal Meetup",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "course/syllabus.html",
    "href": "course/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Instructors: Jason Bryer, Ph.D. and Angela Lui, Ph.D.\nClass Meetup: Wednesday 8:00pm to 9:00pm\nOffice Hours: By appointment\nEmail: jason.bryer@cuny.edu, angela.lui@cuny.edu\n\nCourse Description\nThis course covers basic techniques in probability and statistics that are important in the field of data analytics. Discrete probability models, sampling from infinite and finite populations, statistical distributions, basic Bayesian statistics, and non-parametric statistical techniques for categorical data are covered in this course. Each of these statistical concepts will be applied in a variety of real-world scenarios through the use of case studies and customized data sets.\n\n\nCourse Learning Outcomes:\nBy then end of the course, students should be able to:\n\nUnderstand the foundations of probability theory and perform basic probability calculations.\nBuild basic stochastic models for commonly encountered business problems.\nModel situations involving uncertainty using appropriate probability distributions and conditional techniques.\nExplore and summarize data using descriptive statistics.\nTest hypotheses using classical and modern computational techniques.\nConstruct estimators and calculate intervals using classical and modern computational techniques.\nPerform basic Bayesian statistical techniques for estimation and testing hypotheses.\n\n\n\nProgram Learning Outcomes addressed by the course:\n\nBusiness Understanding: Learn when probabilistic techniques apply to certain categories of business problems, discuss the sorts of solutions that are possible, and understand the limitations of these techniques.\nFoundational Math Skills: Explore and analyze data, build probabilistic and statistical models, construct estimators, and test hypotheses.\nPredictive Modeling: Learn foundational techniques that underlie predictive modeling algorithms, such as Naïve Bayes.\nPresentation: Complete and submit collaborative assignments using techniques from the course.\n\n\n\nHow is this course relevant for data analytics professionals?\nProbabilistic techniques are the foundation of many data science applications from data exploration and visualization to outlier analysis, stochastic modeling, and data mining algorithms. This course will ensure that students have a strong understanding of these foundations.\n\n\nGrading\n\nParticipation (10%)\n\nDAACS\nOne Minute Papers\n\nLabs (35%)\nData Project (30%)\nExams\n\nMidterm (10%)\nFinal exam (15%)\n\n\n\nGrade Distribution\n\n\n\n\n\n\n\n\n\nQuality of Performance\nLetter Grade\nRange %\nGPA\n\n\n\n\nExcellent - work is of exceptional quality\nA\n93 - 100\n4\n\n\nExcellent\nA-\n90 - 92.9\n3.7\n\n\nGood - work is above average\nB+\n87 - 89.9\n3.3\n\n\nSatisfactory\nB\n83 - 86.9\n3\n\n\nBelow Average\nB-\n80 - 82.9\n2.7\n\n\nPoor\nC+\n77 - 79.9\n2.3\n\n\nPoor\nC\n70 - 76.9\n2\n\n\nFailure\nF\n&lt; 70\n0\n\n\n\n\n\n\nHow This Course Works\nThis course is conducted entirely online. Each week, you will have various resources made available, including weekly readings from the textbooks and occasionally additional readings provided by the instructor. Most weeks will have homework assignments and labs to be submitted (although some chapters will take more than one week, see the schedule for details). There will also be a presentation required and a forum post introduction required. You are expected to complete all assignments by their due dates.\nYou are expected to attend or watch every Meetup. We highly recommend attending the Meetups live if possible but understand that may not be possible for everyone. Recordings will be made available by the next morning on the Meetups page. In addition to highlighting key concepts from each learning module, some topics will be discussed that are not in the textbook. Moreover, we regularly make announcements in the Meetups that will be important to being successful in this course. At the end of each Meetup there will be a short reflective exercise. These will contribute to your participation grade.\nThe culmination of the course will be the presentation of the analysis of a dataset of your choosing. There will be a number of time slots available to present. You will be required to attend one presentation session, during which you will present your analysis and provide peer feedback for other students in that timeslot. See the project for more information.\n\n\nAccessibility and Accommodations\nThe CUNY School of Professional Studies is firmly committed to making higher education accessible to students with disabilities by removing architectural barriers and providing programs and support services necessary for them to benefit from the instruction and resources of the University. Early planning is essential for many of the resources and accommodations provided. Please see: http://sps.cuny.edu/student_services/disabilityservices.html\n\n\nOnline Etiquette and Anti-Harassment Policy\nThe University strictly prohibits the use of University online resources or facilities, including Blackboard, for the purpose of harassment of any individual or for the posting of any material that is scandalous, libelous, offensive or otherwise against the University’s policies. Please see: http://media.sps.cuny.edu/filestore/8/4/9_d018dae29d76f89/849_3c7d075b32c268e.pdf\n\n\nAcademic Integrity\nAcademic dishonesty is unacceptable and will not be tolerated. Cheating, forgery, plagiarism and collusion in dishonest acts undermine the educational mission of the City University of New York and the students’ personal and intellectual growth. Please see: http://media.sps.cuny.edu/filestore/8/3/9_dea303d5822ab91/839_1753cee9c9d90e9.pdf\n\n\nStudent Support Services\nIf you need any additional help, please visit Student Support Services: http://sps.cuny.edu/student_resources/",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course/materials.html",
    "href": "course/materials.html",
    "title": "Materials",
    "section": "",
    "text": "These are supplemental materials. Click to download.\n\nChapter 28_Anova.pdf\nCheatsheets_2019.pdf\nClassical_Machine_Learning.JPG\nElementalGraphics4ANOVA.RP+JH.pdf\nggplot_aesthetics_cheatsheet.png\nKruschke-Liddell2018_Article_BayesianDataAnalysisForNewcome.pdf\nR_Syntax_Comparison.jpeg\nRiezler2021_EmpiricalMethods-draft.pdf\nstats_handout.pdf\nTextbooks\n\nlsr-0.6.pdf\nos4.pdf\nProbStatBook.pdf\nSupplemental\n\nOpenIntro_extra_interaction_effects.pdf\nOpenIntro_extra_nonlinear_relationships.pdf\nOpenIntro_handout.pdf\nos2_extra_inference_guide.pdf\nos2_prob_tables.pdf\nos4_tablet.pdf\n\n\nThirteen Ways to Look at the Correlation Coefficient.pdf\nWhat_Is_Data_Science.pdf\nWhatisDataScienceFinalMay162018.pdf\nWhen to use what test.pdf",
    "crumbs": [
      "Course information",
      "Materials"
    ]
  },
  {
    "objectID": "course/overview.html",
    "href": "course/overview.html",
    "title": "DATA606 - Statistics and Probability",
    "section": "",
    "text": "This course covers basic techniques in probability and statistics that are important in the field of data analytics. Discrete probability models, sampling from infinite and finite populations, statistical distributions, basic Bayesian statistics, and non-parametric statistical techniques for categorical data are covered in this course. Each of these statistical concepts will be applied in a variety of real-world scenarios through the use of case studies and customized data sets."
  },
  {
    "objectID": "course/instructors.html",
    "href": "course/instructors.html",
    "title": "Instructors",
    "section": "",
    "text": "Jason Bryer, Ph.D.\nEmail: jason.bryer@cuny.edu\nI am currently an Assistant Professor in Data Science and Information Systems at the City University of New York. Additionally, I am a consultant with Cornell University where he has developed a research and data collection platform for New York State’s Office of Special Education (https://data.osepartnership.org). Prior to joining CUNY, I was Executive Director at Excelsior College where he has served as Principal Investigator of the FIPSE First in the World grant to develop and research the Diagnostic Assessment and Achievement of College Skills along with supporting research and evaluation of other grants at the institution. My research interests include quasi-experimental designs with an emphasis in propensity score analysis, data systems to support formative assessment, and the use of open source software for conducting reproducible research. I have authored of over a dozen R packages, including three related to conducting propensity score analyses. When I’m not crunching numbers, I am a photographer and proud dad to three boys.\n\nAngela Lui, Ph.D.\nEmail: Angela.Lui@sps.cuny.edu\nI am a Research Project Manager for the Diagnostic Assessment and Achievement of College Skills, a five year multi-million-dollar IES grant funded project. I graduated with my doctoral degree in Educational Psychology and Methodology at the University at Albany – SUNY in 2020, and served as a postdoctoral associate for a year at Rutgers University. For my dissertation, I explored students’ interpretations and feelings about their teacher’s feedback within a formative assessment context. More broadly, my research interests include formative assessment and self-regulated learning, as well as applied measurement and statistics. A fun fact: I grew up in Manhattan, NY, one of the busiest cities, but am now living happily on a farm with my chosen family, helping to take care of trees, gardens, barns, chickens, a cat, and a dog.\n\nGeorge Hagstorm, Ph.D.\nEmail: george.hagstrom@cuny.edu\nI am currently a Doctoral Lecturer in Data Science and Information Systems at the City University of New York. Additionally, in collaboration with scientists at Princeton University I develop mathematical models of marine phytoplankton and heterotrophic bacteria to improve our understanding of the Earth’s Biogeochemical Cycles. Prior to joining CUNY, I was a Research Scientist at Princeton University in the Levin lab at the Department of Ecology and Evolutionary Biology and a postdoctoral researcher in the Magneto Fluids Division at the Courant Institute for Mathematical Sciences. My research interests include the application of Bayesian statistics and Machine Learning to incorporate nontraditional datasets (such as from ’omics) into mechanistic models of marine ecosystems and biogeochemical cycling, trait-based modeling, and critical transitions in complex ecological, social, or economic systems. In my free time, I enjoy cycling and exploring New York City.\n\nContact\nOffice Hours (Zoom is preferred): By appointment. You’re encouraged to schedule an appointment and I have time nearly everyday. You are also encouraged to ask us questions on Slack. If you wish to ask a question in private, you can email me directly.\nFor the most part, you can expect me to respond to questions by email within 24 hours. If you do not hear back from me within 48 hours of sending an email, please resend your message. I will be checking in on the course regularly, just about every day and likely several times each day. Please do not hesitate to ask if you have questions or concerns.",
    "crumbs": [
      "Course information",
      "Instructors"
    ]
  },
  {
    "objectID": "course/software.html",
    "href": "course/software.html",
    "title": "Software",
    "section": "",
    "text": "R and RStudio\n\nWe will make use of R, an open source statistics program and language. Be sure to install R and RStudio on your own computers within the first few days of the class.\n\nR - download for Windows, Mac, or Linux.\nRStudio - Download Windows, Mac, or Linux versions from here\n\nIf using Windows, you also need to download RTools.\n\n\nLaTeX\n\nLaTeX is a typesetting language for preparing documents. Documents are written in plain text files. Formatting the document is done using specific markup. If you have used HTML, the framework is similar however instead of using &lt;TAG&gt;&lt;/TAG&gt; syntax, LaTeX uses \\TAG{} format. We will primarily use Markdown, and its extension R Markdown for preparing documents in this class. However, when preparing PDF documents, the Markdown will first be converted to LaTeX before creating the PDF file. As such, a LaTeX converter is necessary. There are LaTeX installers for Windows (MiKTeX) and Mac (BasicTeX). Alternatively, the tinytex R package provides an easier way of installing LaTeX directly from within R:\n\ninstall.packages('tinytex')\ntinytex::install_tinytex()\n\n\n\nSource Control\nAll course materials will be made available on Github which provides an implementation of the git open source version control system. RStudio supports git directly. For more information, Jenny Bryan’s Happy Git and Github for the useR is a free online book covering the important features of source control for R users.\n\n\nR Packages\n\nOnce everything is installed, execute the following command in RStudio to install the packages we will use for this class (you can copy-and-paste):\n\ninstall.packages(c('openintro','devtools','tidyverse', 'ggplot2',\n                   'psych','reshape2','knitr','markdown','shiny','R.rsp',\n                   'fivethirtyeight'))\nremotes::install_github(\"jbryer/DATA606\")\nremotes::install_github(\"jbryer/VisualStats\")\n\n\n\nThe DATA606 R Package\n\nMany of the course resouces are available in the DATA606 R package. Here are some command to get started:\n\nlibrary('DATA606')          # Load the package\nvignette(package='DATA606') # Lists vignettes in the DATA606 package\nvignette('os4')             # Loads a PDF of the OpenIntro Statistics book\ndata(package='DATA606')     # Lists data available in the package\ngetLabs()                   # Returns a list of the available labs\nviewLab('Lab1')             # Opens Lab1 in the default web browser\nstartLab('Lab1')            # Starts Lab1 (copies to getwd()), opens the Rmd file\nshiny_demo()                # Lists available Shiny apps",
    "crumbs": [
      "Course information",
      "Software"
    ]
  },
  {
    "objectID": "course/textbooks.html",
    "href": "course/textbooks.html",
    "title": "Textbooks",
    "section": "",
    "text": "Required\n\nDiez, D.M., Barr, C.D., & Çetinkaya-Rundel, M. (2019). OpenIntro Statistics (4th Ed).\n\nThis is an open source textbook and can be downloaded in PDF format here, from the OpenIntro website, or a printed copy can be ordered from Amazon.\n\n \nNavarro, D. (2018, version 0.6). Learning Statistics with R\n\nThis is free textbook that supplements a lot of the material covered in Diez and Barr. We will use the chapter on Bayesian analysis. You can download a PDF version, Bookdown version, or visit the author’s website at learningstatisticswithr.com.\n\n\n\nRecommended\nOpenStax.org is a great resource for free textbooks. The following books will be helpful to have as a reference and to supplement, and get an alternative explanition, for many of the topics covered in this course:\n\nStatstics\nCalculus - We will very briefly explain the concepts of limits, derivatives, and integrals that underlie some important statistical concepts. This books will provide much more detail.\nCollege Algebra - For those who need a refresher in algebra, this is a good resource.\n\nWickham, H., & Grolemund, G. (2016) R for Data Science. O’Reilly.\n\nMost of this books is available freely online at r4ds.had.co.nz/ but can be purchased from Amazon.\n\nWickham, H. Advanced R. Baca Raton, FL: Taylor & Francis Group.\n\nMost of this book is available freely online at adv-r.had.co.nz but can be purchased from Amazon.\n\nKruschke, J.K. (2014). Doing Bayesian Data Analysis, Second Edition: A Tutorial with R, JAGS, and Stan (2nd Ed). London: Academic Press.\n\nThis book can be purchased from Amazon, but also check out the author’s webiste (doingbayesiandataanalysis.blogspot.com/) for additional resources.",
    "crumbs": [
      "Course information",
      "Textbooks"
    ]
  },
  {
    "objectID": "course/calendar.html",
    "href": "course/calendar.html",
    "title": "DATA606 - Statistics and Probability",
    "section": "",
    "text": "Note: Schedule is subject to change. Last updated June 30, 2024 10:55AM.\nCUNY SPS Academic Calendar\n\n\n\n\nClick here to import the course calendar into your calendar application"
  },
  {
    "objectID": "assignments/labs.html",
    "href": "assignments/labs.html",
    "title": "Labs",
    "section": "",
    "text": "These mini projects will have you explore statistical topics using R. You can use the startLab function in the DATA606 package to get started, or copy the templates from the links below. Please submit a PDF (preferred) or HTML file along with your Rmarkdown file. Be sure to answer all questions in lab, not just the on your own section. Labs should be submitted on Blackboard.\n\n\nIntroduction to R and RStudio (Template)\n\n\nIntroduction to Data (Template)\n\n\nProbability (Template)\n\n\nDistributions of Random Variables (Template)\n\n\nFoundations for Statistical Inference\n\n\nSampling Distributions (Template)\n\n\nConfidence Levels (Template)\n\n\n\n\nInference for Categorical Data (Template)\n\n\nInference for Numerical Data (Template)\n\n\nIntroduction to Linear Regression (Template)\n\n\nMultiple Linear Regerssion (Template)",
    "crumbs": [
      "Assignments",
      "Labs"
    ]
  },
  {
    "objectID": "assignments/project.html",
    "href": "assignments/project.html",
    "title": "Project",
    "section": "",
    "text": "The purpose of the data project is for you to conduct a reproducible analysis with a data set of your choosing. There are two components to the project, the proposal, which will be graded on a pass/fail basis, and the final report. The outline for each of these are provided in the templates. When submitting the assignments, include the R Markdown file (change the name to include your last name, for example Bryer-Proposal.Rmd and Bryer-Project.Rmd) along with any supplementary files necessary to run the R Markdown file (e.g. data files, screenshots, etc.). Suggestions for possible data sources are included below, however you are free to use data not listed below. The only requirement is that you are allowed to share the data. Projects will be shared with others on this website so should be presented in a way that other students can reproduce your analysis.",
    "crumbs": [
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/project.html#project-proposal",
    "href": "assignments/project.html#project-proposal",
    "title": "Project",
    "section": "Project Proposal",
    "text": "Project Proposal\nThe proposal can be more informal using bullet points where necessary and include R code and output. You must address the following areas:\n\nResearch question\nWhat are the cases, and how many are there?\nDescribe the method of data collection.\nWhat type of study is this (observational/experiment)?\nData Source: If you collected the data, state self-collected. If not, provide a citation/link.\nResponse: What is the response variable, and what type is it (numerical/categorical)?\nExplanatory: What is the explanatory variable(s), and what type is it (numerical/categorical)?\nRelevant summary statistics\nDownload project proposal template\nDownload project template\n\nExample data project proposal (Source Rmarkdown file)",
    "crumbs": [
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/project.html#final-project",
    "href": "assignments/project.html#final-project",
    "title": "Project",
    "section": "Final Project",
    "text": "Final Project\n\nYou are required to attend ONLY ONE of those time slots. You will do your presentation, watch the other presentations, and provide peer feedback (will be shared anonymously afterward).\n\nExample project slide deck\n\nChecklist / Suggested Outline\n\nAbstract (no more than 300 words)\nOverview slide\n\nContext on the data collection\nDescription of the dependent variable (what is being measured)\nDescription of the independent variable (what is being measured; include at least 2 variables)\nResearch question\n\nSummary statistics\nInclude appropriate data visualizations.\nStatistical output\n\nInclude the appropriate statistics for your method used.\nFor null hypothesis tests (e.g. t-test, chi-squared, ANOVA, etc.), state the null and alternative hypotheses along with relevant statistic and p-value (and confidence interval if appropriate).\nFor regression models, include the regression output and interpret the R-squared value.\n\nConclusion\n\nWhy is this analysis important?\nLimitations of the analysis?\n\n\n\n\nRubric\n\n\n\n\n\n\n\n\n\nDomain\nAccomplished\nProficient\nNeeds Improvement\n\n\n\n\nAbstract\nAbstract is less than 300 words, free of grammatical errors, summarizes the analysis conducted, has a conclusion and implicaitons\nNA\nNA\n\n\nIntroduction\nThe research question is clearly stated, can be answered by the data, and the context of the problem clearly explained.\nThe research question is unclear and/or not supported by the data.\nResearch question is ambiguous, unclear, or not stated.\n\n\nData Display\nIncludes appropriate, well-labeled, accurate displays (graphs and tables) of the data.\nIncludes appropriate, accurate displays of the data.\nIncludes appropriate but no accurate displays of the data.\n\n\nData Analysis\nThe appropriate statistical test(s) was used for the data and interpretation was clear.\nThe appropriate statistical test(s) was used but interpretation was not fully clear or well articulated.\nThe incorrect statistical test was used an/or not justified for the data as presented.\n\n\nConclusion\nConclusion includes a clear answer to the statistical question that is consistent with the data analysis and the method of data collection.\nConclusion includes an answer to the statistical question that is consistent with the data but not with the data collection method.\nConclusion does not include an answer to the statistical question that is consistent with the data analysis.\n\n\nOverall Presentation\nAttractive, well-organized, well-written presentation\nPresentation has two of the three qualities: attractive, well-organized, well-written.\nPresentation is not attractive, organized, or written. There are numerous errors throughout.",
    "crumbs": [
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/project.html#example-data-sources",
    "href": "assignments/project.html#example-data-sources",
    "title": "Project",
    "section": "Example Data Sources",
    "text": "Example Data Sources\nYou are not to use data sources used in class or the textbooks. Possible data sources include, but are not limited to:\n\nFiveThirtyEight https://github.com/fivethirtyeight/data\nRStudio data sources http://blog.rstudio.org/2014/07/23/new-data-packages/\nAnalyze Survey Data for Free (ASDFree) has many open data sources that can be used http://www.asdfree.com/\nThe World Bank Data Catalog http://datacatalog.worldbank.org/\nGoogle Public Data search engine http://www.google.com/publicdata/directory\nVanderbilt data sources http://biostat.mc.vanderbilt.edu/wiki/Main/DataSets\nProgramme of International Student Assessment (PISA) http://www.oecd.org/pisa/\nBehavioral Risk Factor Surveillance System (BRFSS) http://www.cdc.gov/brfss/\nWorld Values Survey http://www.worldvaluessurvey.org/wvs.jsp\nAmerican National Election Survey (ANES) http://www.electionstudies.org/\nGeneral Social Survey (GSS) http://www3.norc.org/GSS+Website/\nIntegrated Postsecondary Education Data System (IPEDS) https://nces.ed.gov/ipeds/\nU.S. Census and American Community Survey https://cran.r-project.org/web/packages/acs/index.html\n10 Standard Datasets for Practicing Applied Machine Learning\nAwesome Public Datasets\nUCI Machine Learning Repository - See also this R package: https://github.com/tyluRp/ucimlr\nOpenML",
    "crumbs": [
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/homework_ungraded.html",
    "href": "assignments/homework_ungraded.html",
    "title": "Homework",
    "section": "",
    "text": "Recommended Homework Questions to Complete for Practice. These will not be graded. The answers are at the end of the textbook. Successfully completing these will help you prepare for the midterm and final exams.\nChapter 1: 1.09 (p. 20), 1.13 (p. 29), 1.27 (p. 31), 1.33 (p. 35)\nChapter 2: 2.33 (p. 78), 2.09 (p. 57), 2.15 (p. 59), 2.25 (p. 76)\nChapter 3: 3.5 (p. 92), 3.7 (p. 93), 3.17 (p. 111), 3.25 (p. 114), 3.33 (p. 124), 3.37 (p, 128)\nChapter 4: 4.1 (p. 142), 4.4 (p. 142), 4.14 (p. 148), 4.29 (p.162)\nChapter 5: 5.3 (179), 5.5 (180), 5.7 (p. 187), 5.9 (p. 187) 5.11 (p. 188), 5.19 (p. 202), 5.25 (p. 203), 5.31 (p. 204), 5.35 (p. 205), 5.37 (p. 205)\nChapter 6: 6.47 (p, 247), 6.09 (p. 216), 6.15 (p. 216), 6.21 (p. 226), 6.33 (p. 239), 6.49 (p. 248)\nChapter 7: 5.23 (p. 203), 7.13, p. 261), 7.19 (p, 266), 7.27 (p.276), 7.33 (p. 284)\nChapter 8: 8.21 (p. 326), 8.13 (p. 316), 8.23 (p. 326), 8.25 (p. 327), 8.43 (p, 340)\nChapter 9: 9.1 (p. 350), 9.3 (p. 352), 9.7 (p, 357), 9.15 (p. 380), 9.17 (p, 381)"
  },
  {
    "objectID": "posts/2024-08-28-Welcome.html",
    "href": "posts/2024-08-28-Welcome.html",
    "title": "Welcome to DATA606!",
    "section": "",
    "text": "Welcome to DATA606! Your instructors will be Dr. Jason Bryer and Dr. Angela Lui. Couple of important notes as you get started:\n\nThe course syllabus located on this website here: https://fall2024.data606.net/course/syllabus.html We will post all course materials there. Blackboard will be used primarily for submitting assignments. Please read the syllabus carefully! Send us any questions you may still have about the course.\nComplete this Google Form as soon as possible. We will use some of the data (in aggregate) in the meetup.\nJoin the Slack channel by clicking here (or use your @sps.cuny.edu email address). This will be our primary mode of communication.\nFor private communications, you can either send private Slack messages or email us at jason.bryer@cuny.edu, angela.lui@cuny.edu, and george.hagstrom@cuny.edu.\nComplete the self-regulated learning assessment in DAACS. Visit /assignments/daacs/ for more information.\nOnce you go through the syllabus, try starting Lab 1 as soon as you can. This will require you to install R and RStudio and will help get you get acquainted with R. See the software page for more information about the software we will use for this course.\nThe best way to reach us is on Slack. If you would prefer to talk on the phone or setup a Zoom session, it is best to suggest some times by email first.\nOur Meetups will be on Wednesdays from 8:00pm to 9:00pm. We expect everyone to attend or watch the Meetups. Important information will be shared during the meetups that will not be available in any other format.\n\nWe am looking forward to getting to know everyone and a fantastic semester! Good luck!"
  },
  {
    "objectID": "chapters/chapter2.html",
    "href": "chapters/chapter2.html",
    "title": "Chapter 2 - Summarizing Data",
    "section": "",
    "text": "Learning Outcomes\n\nUse appropriate visualizations for different types of data (e.g. histogram, barplot, scatterplot, boxplot, etc.).\nUse different measures of center and spread and be able to describe the robustness of different statistics.\nDescribe the shape of distributions vis-a-vis histograms and boxplots.\nCreate and intepret contingency and frequency tables (one- and two-way tables).\n\n\n\nSupplemental Readings\n\nOpenIntro Statistics slides\nggplot2 - ggplot2 is an R package by Wickham that implements the grammer of graphics (Wilkinson, 2005) in R. I will frequently make use of the graphing framework throughout the course and is worth learning.\nVisualizing Likert Data - An R package for visualizing Likert scale data built on the ggplot2 framework.\nQuick-R base graphics - Covers many of the visualizations using R’s base graphics.\n\n\n\nVideos\nSummarizing and Graphing Numerical Data\n\n\nExploring Categorical Data\n\n\n\n\nNote about Pie Charts\nThere is only one pie chart in OpenIntro Statistics (Diez, Barr, & ??etinkaya-Rundel, 2015, p. 48). Consider the following three pie charts that represent the preference of five different colors. Is there a difference between the three pie charts? This is probably a difficult to answer.\n\n\n\nPie\n\n\nHowever, consider the bar plot below. Here, we cleary see there is a difference between the ratio of the three colors. As John Tukey famously said:\n\nThere is no data that can be displayed in a pie chart that cannot better be displayed in some other type of chart\n\n\n\n\nBar\n\n\nSource: https://en.wikipedia.org/wiki/Pie_chart.",
    "crumbs": [
      "Topics",
      "2 - Summarizing Data"
    ]
  },
  {
    "objectID": "chapters/chapter5.html",
    "href": "chapters/chapter5.html",
    "title": "Chapter 5 - Foundations for Inference",
    "section": "",
    "text": "Learning Outcomes\n\nDefine sample statistic as a point estimate for a population parameter, for example, the sample proportion is used to estimate the population proportion, and note that point estimate and sample statistic are synonymous.\nRecognize that point estimates (such as the sample proportion) will vary from one sample to another, and define this variability as sampling variation.\nCalculate the sampling variability of the proportion, the standard error, as \\(SE = \\sqrt{\\frac{p(1-p)}{n}}\\), where \\(p\\) is the population proportion.\n\nNote that when the population proportion \\(p\\) is not known (almost always), this can be estimated using the sample proportion, \\(SE = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\).\n\nStandard error measures the variability in point estimates from different samples of the same size and from the same population, i.e. measures the sampling variability.\nRecognize that when the sample size increases we would expect the sampling variability to decrease.\n\nConceptually: Imagine taking many samples from the population. When sample sizes are large the sample proportion will be much more consistent across samples than when the sample sizes are small.\nMathematically: \\(SE = ???\\), when \\(n\\) increases, \\(SE\\) will decrease since \\(n\\) is in the denominator.\n\nNotice that sampling distributions of point estimates coming from samples that don’t meet the required conditions for the CLT (about sample size and independence) will not be normal.\nDefine a confidence interval as the plausible range of values for a population parameter.\nDefine the confidence level as the percentage of random samples which yield confidence intervals that capture the true population parameter.\nCalculate an approximate 95% confidence interval by adding and subtracting 2 standard errors to the point estimate: \\(point~estimate \\pm 2 \\times SE\\).\nRecognize that the Central Limit Theorem (CLT) is about the distribution of point estimates, and that given certain conditions, this distribution will be nearly normal.\n\nIn the case of the proportion the CLT tells us that if\n\nthe observations in the sample are independent, and\nthere are at least 10 successes and 10 failures, \\end{itemize} then the distribution of the sample proportion will be nearly normal, centered at the true population proportion and with a standard error of \\(\\sqrt{\\frac{p(1-p)}{n}}\\). [ N ( mean = p, SE = ) ]\n\nWhen the population proportion is unknown, condition (2) can be checked using the sample proportion.\n\nRecall that independence of observations in a sample is provided by random sampling (in the case of observational studies) or random assignment (in the case of experiments).\n\nIn addition, the sample should not be large compared to the population, or more precisely, should be smaller than 10% of the population, since samples that are too large will likely contain observations that are not independent. \\end{itemize}\n\nRecognize that the nearly normal distribution of the point estimate (as suggested by the CLT) implies that a more precise confidence interval can be calculated as [ point~estimate z^{} SE, ] where \\(z^{\\star}\\) corresponds to the cutoff points in the standard normal distribution to capture the middle XX% of the data, where XX% is the desired confidence level.\n\nFor proportions this is \\(\\bar{x} \\pm Z^\\star \\sqrt{\\frac{p(1-p)}{n}}\\).\nNote that \\(z^{\\star}\\) is always positive.\n\nDefine margin of error as the distance required to travel in either direction away from the point estimate when constructing a confidence interval, i.e. \\(z^{\\star} \\times SE\\).\n\nNotice that this corresponds to half the width of the confidence interval.\n\nInterpret a confidence interval as “We are XX% confident that the true population parameter is in this interval”, where XX% is the desired confidence level.\n\nNote that your interpretation must always be in context of the data – mention what the population is and what the parameter is (mean or proportion).\n\nExplain how the hypothesis testing framework resembles a court trial.\nRecognize that in hypothesis testing we evaluate two competing claims:\n\nthe null hypothesis, which represents a skeptical perspective or the status quo, and\nthe alternative hypothesis, which represents an alternative under consideration and is often represented by a range of possible parameter values.\n\nConstruction of hypotheses:\n\nAlways construct hypotheses about population parameters (e.g. population proportion, \\(p\\)) and not the sample statistics (e.g. sample proportion, \\(\\hat{p}\\)). Note that the population parameter is unknown while the sample statistic is measured using the observed data and hence there is no point in hypothesizing about it.\nDefine the null value as the value the parameter is set to equal in the null hypothesis.\nNote that the alternative hypothesis might be one-sided (\\(\\mu\\) \\(&lt;\\) or \\(&gt;\\) the null value) or two-sided (\\(\\mu\\) \\(\\ne\\) the null value), and the choice depends on the research question.\n\nDefine a p-value as the conditional probability of obtaining a sample statistic at least as extreme as the one observed given that the null hypothesis is true. $\\text{p-value} = \\text{P(observed or more extreme sample statistic}~|~H_0 \\text{ true)}$\nCalculate a p-value as the area under the normal curve beyond the observed sample proportion (either in one tail or both, depending on the alternative hypothesis). Note that in doing so you can use a Z score, where $Z = \\frac{sample~statistic - null~value}{SE} = \\frac{\\bar{x} - \\mu_0}{SE}$\n\nAlways sketch the normal curve when calculating the p-value, and shade the appropriate area(s) depending on whether the alternative hypothesis is one- or two-sided.\n\nInfer that if a confidence interval does not contain the null value the null hypothesis should be rejected in favor of the alternative.\nCompare the p-value to the significance level to make a decision between the hypotheses:\n\nIf the p-value \\(&lt;\\) the significance level, reject the null hypothesis since this means that obtaining a sample statistics at least as extreme as the observed data is extremely unlikely to happen just by chance, and conclude that the data provides evidence for the alternative hypothesis.\nIf the p-value \\(&gt;\\) the significance level, fail to reject the null hypothesis since this means that obtaining a sample statistics at least as extreme as the observed data is quite likely to happen by chance, and conclude that the data does not provide evidence for the alternative hypothesis.\nNote that we can never “accept” the null hypothesis since the hypothesis testing framework does not allow us to confirm it.\n\nNote that the conclusion of a hypothesis test might be erroneous regardless of the decision we make.\n\nDefine a Type 1 error as rejecting the null hypothesis when the null hypothesis is actually true.\nDefine a Type 2 error as failing to reject the null hypothesis when the alternative hypothesis is actually true.\n\nChoose a significance level depending on the risks associated with Type 1 and Type 2 errors.\n\nUse a smaller \\(\\alpha\\) is Type 1 error is relatively riskier.\nUse a larger \\(\\alpha\\) is Type 2 error is relatively riskier.\n\nFormulate the framework for statistical inference using hypothesis testing and nearly normal point estimates:\n\nSet up the hypotheses first in plain language and then using appropriate notation.\nIdentify the appropriate sample statistic that can be used as a point estimate for the parameter of interest.\nVerify that the conditions for the CLT holds.\nCompute the SE, sketch the sampling distribution, and shade area(s) representing the p-value.\nUsing the sketch and the normal model, calculate the p-value and determine if the null hypothesis should be rejected or not, and state your conclusion in context of the data and the research question.\n\nIf the conditions necessary for the CLT to hold are not met, note this and do not go forward with the analysis. (We will later learn about methods to use in these situations.)\nDistinguish statistical significance vs. practical significance.\n\n\n\nSupplemental Readings\n\nOpenIntro Statistics slides\nWhy do we use 0.05 as a significance level?\n\n\n\nVideos",
    "crumbs": [
      "Topics",
      "5 - Foundations for Inference"
    ]
  },
  {
    "objectID": "chapters/chapter6.html",
    "href": "chapters/chapter6.html",
    "title": "Chapter 6 - Inference for Categorical Data",
    "section": "",
    "text": "Learning Outcomes\n\nDefine population proportion \\(p\\) (parameter) and sample proportion \\(\\hat{p}\\) (point estimate).\nCalculate the sampling variability of the proportion, the standard error, as [ SE = , ] where \\(p\\) is the population proportion.\n\nNote that when the population proportion \\(p\\) is not known (almost always), this can be estimated using the sample proportion, \\(SE = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\).\n\nRecognize that the Central Limit Theorem (CLT) is about the distribution of point estimates, and that given certain conditions, this distribution will be nearly normal.\n\nIn the case of the proportion the CLT tells us that if \\\n\nthe observations in the sample are independent, \\\nthe sample size is sufficiently large (checked using the success/failure condition: \\(np \\ge 10\\) and \\(n(1-p) \\ge 10\\)), \\ then the distribution of the sample proportion will be nearly normal, centered at the true population proportion and with a standard error of \\(\\sqrt{\\frac{p(1-p)}{n}}\\). [ N ( mean = p, SE = ) ]\n\n\nNote that if the CLT doesn?t apply and the sample proportion is low (close to 0) the sampling distribution will likely be right skewed, if the sample proportion is high (close to 1) the sampling distribution will likely be left skewed.\nRemember that confidence intervals are calculated as [ ] and test statistics are calculated as [ ]\nNote that the standard error calculation for the confidence interval and the hypothesis test are different when dealing with proportions, since in the hypothesis test we need to assume that the null hypothesis is true – remember: p-value = P(observed or more extreme test statistic \\(|\\) \\(H_0\\) true).\n\nFor confidence intervals use \\(\\hat{p}\\) (observed sample proportion) when calculating the standard error and when checking the success/failure condition: $SE_{\\hat{p}} = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}$\nFor hypothesis tests use \\(p_0\\) (null value) when calculating the standard error and checking the success/failure condition: $SE_{\\hat{p}} = \\sqrt{\\frac{p_0 (1-p_0)}{n}}$\nSuch a discrepancy doesn’t exist when conducting inference for means, since the mean doesn’t factor into the calculation of the standard error, while the proportion does.\n\nCalculate the required minimum sample size for a given margin of error at a given confidence level, and explain why we use \\(\\hat{p} = 0.5\\) if there are no previous studies suggesting a more accurate estimate.\n\nConceptually: When there is no additional information, 50% chance of success is a good guess for events with only two outcomes (success or failure).\nMathematically: Using \\(\\hat{p} = 0.5\\) yields the most conservative (highest) estimate for the required sample size.\n\nNote that the calculation of the standard error of the distribution of the difference in two independent sample proportions is different for a confidence interval and a hypothesis test.\n\nconfidence interval (and hypothesis test when $H_0: p_1 -p_2 =$ some value other than 0): $SE_{(\\hat{p}_1 - \\hat{p}_2)} = \\sqrt{\\frac{ \\hat{p}_1 (1 - \\hat{p}_1)}{n_1} + \\frac{ \\hat{p}_2 (1 - \\hat{p}_2)}{n_2} }$\nhypothesis test when $H_0: p_1 -p_2 = 0$: $SE_{(\\hat{p}_1 - \\hat{p}_2)} = \\sqrt{\\frac{ \\hat{p}_{pool} (1 - \\hat{p}_{pool})}{n_1} + \\frac{ \\hat{p}_{pool} (1 - \\hat{p}_{pool})}{n_2} }$, where \\(\\hat{p}_{pool}\\) is the overall rate of success: $\\hat{p}_{pool} = \\frac{\\text{number of successes in group 1 + number of successes in group 2}}{n_1 + n_2}$\n\nNote that the reason for the difference in calculations of standard error is the same as in the case of the single proportion: when the null hypothesis claims that the two population proportions are equal, we need to take that into consideration when calculating the standard error for the hypothesis test, and use a common proportion for both samples.\nUse a chi-square test of goodness of fit to evaluate if the distribution of levels of a single categorical variable follows a hypothesized distribution.\n\n\\(H_0:\\) The distribution of the variable follows the hypothesized distribution, and any observed differences are due to chance.\n\\(H_A:\\) The distribution of the variable does not follow the hypothesized distribution.\n\nCalculate the expected counts for a given level (cell) in a one-way table as the sample size times the hypothesized proportion for that level.\nCalculate the chi-square test statistic as $\\chi = \\sum_{i = 1}^{k}  \\frac{(\\text{observed count} - \\text{expected count})^2}{\\text{expected count}}$, where \\(k\\) is the number of cells.\nNote that the chi-square distribution is right skewed with one parameter: degrees of freedom. In the case of a goodness of fit test, \\(df = \\# \\text{of categories} - 1\\).\nList the conditions necessary for performing a chi-square test (goodness of fit or independence)\n\nthe observations should be independent\nexpected counts for each cell should be at least 5\ndegrees of freedom should be at least 2 (if not, use methods for evaluating proportions)\n\nDescribe how to use the chi-square table to obtain a p-value.\nWhen evaluating the independence of two categorical variables where at least one has more than two levels, use a chi-square test of independence.\n\n\\(H_0:\\) The two variables are independent.\n\\(H_A:\\) The two variables are dependent.\n\nCalculate expected counts in two-way tables as [ E = ]\nCalculate the degrees of freedom for chi-square test of independence as \\(df = (R - 1) \\times (C - 1)\\), where \\(R\\) is the number of rows in a two-way table, and \\(C\\) is the number of columns.\nNote that there is no such thing as a chi-square confidence interval for proportions, since in the case of a categorical variables with many levels, there isn’t one parameter to estimate.\nUse simulation methods when sample size conditions aren’t met for inference for categorical variables.\n\nNote that the \\(t\\)-distribution is only appropriate to use for means, when sample size isn’t sufficiently large, and the parameter of interest is a proportion or a difference between two proportions, we need to use simulation.\n\nIn hypothesis testing\n\nfor one categorical variable, generate simulated samples based on the null hypothesis, and then calculate the number of samples that are at least as extreme as the observed data.\nfor two categorical variables, use a randomization test.\n\nUse bootstrap methods for confidence intervals for categorical variables with at most two levels.\n\n\n\nSupplemental Readings\n\nOpenIntro Statistics slides\n\n\n\nVideos",
    "crumbs": [
      "Topics",
      "6 - Inference for Categorical Data"
    ]
  },
  {
    "objectID": "chapters/chapter9.html",
    "href": "chapters/chapter9.html",
    "title": "Chapter 9 - Multiple and Logistic Regression",
    "section": "",
    "text": "Learning Outcomes\n\nDefine the multiple linear regression model as $$\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k$$ where there are \\(k\\) predictors (explanatory variables).\nInterpret the estimate for the intercept (\\(b_0\\)) as the expected value of \\(y\\) when all predictors are equal to 0, on average.\nInterpret the estimate for a slope (say \\(b_1\\)) as “All else held constant, for each unit increase in \\(x_1\\), we would expect \\(y\\) to increase/decrease on average by \\(b_1\\).”\nDefine collinearity as a high correlation between two independent variables such that the two variables contribute redundant information to the model – which is something we want to avoid in multiple linear regression.\nNote that \\(R^2\\) will increase with each explanatory variable added to the model, regardless of whether or not the added variables is a meaningful predictor of the response variable. Therefore we use adjusted \\(R^2\\), which applies a penalty for the number of predictors included in the model, to better assess the strength of a multiple linear regression model: $$R^2 = 1 - \\frac{Var(e_i) / (n - k - 1)}{Var(y_i) / (n - 1)}$$ where $Var(e_i)$ measures the variability of residuals ($SS_{Err}$), $Var(y_i)$ measures the total variability in observed \\(y\\) ($SS_{Tot}$), \\(n\\) is the number of cases and \\(k\\) is the number of predictors.\n\nNote that adjusted \\(R^2\\) will only increase if the added variable has a meaningful contribution to the amount of explained variability in \\(y\\), i.e. if the gains from adding the variable exceeds the penalty.\n\nDefine model selection as identifying the best model for predicting a given response variable.\nNote that we usually prefer simpler (parsimonious) models over more complicated ones.\nDefine the full model as the model with all explanatory variables included as predictors.\nNote that the p-values associated with each predictor are conditional on other variables being included in the model, so they can be used to assess if a given predictor is significant, given that all others are in the model.\n\nThese p-values are calculated based on a \\(t\\) distribution with \\(n - k - 1\\) degrees of freedom.\nThe same degrees of freedom can be used to construct a confidence interval for the slope parameter of each predictor: $$b_i \\pm t^\\star_{n - k - 1} SE_{b_i}$$\n\nStepwise model selection (backward or forward) can be done based based on adjusted \\(R^2\\) (choose the model with higher adjusted \\(R^2\\)).\nThe general idea behind backward-selection is to start with the full model and eliminate one variable at a time until the ideal model is reached.\n\nStart with the full model.\nRefit all possible models omitting one variable at a time, and choose the model with the highest adjusted \\(R^2\\).\nRepeat until maximum possible adjusted \\(R^2\\) is reached.\n\nThe general idea behind forward-selection is to start with only one variable and adding one variable at a time until the ideal model is reached.\n\nTry all possible simple linear regression models predicting \\(y\\) using one explanatory variable at a time. Choose the model with the highest adjusted \\(R^2\\).\nTry all possible models adding one more explanatory variable at a time, and choose the model with the highest adjusted \\(R^2\\).\nRepeat until maximum possible adjusted \\(R^2\\) is reached.\n\nAdjusted \\(R^2\\) method is more computationally intensive, but it is more reliable, since it doesn’t depend on an arbitrary significant level.\nList the conditions for multiple linear regression as\n\nlinear relationship between each (numerical) explanatory variable and the response - checked using scatterplots of \\(y\\) vs. each \\(x\\), and residuals plots of \\(residuals\\) vs. each \\(x\\)\nnearly normal residuals with mean 0 - checked using a normal probability plot and histogram of residuals\nconstant variability of residuals - checked using residuals plots of \\(residuals\\) vs. \\(\\hat{y}\\), and \\(residuals\\) vs. each \\(x\\)\nindependence of residuals (and hence observations) - checked using a scatterplot of \\(residuals\\) vs. order of data collection (will reveal non-independence if data have time series structure)\n\nNote that no model is perfect, but even imperfect models can be useful.\n\n\n\nSupplemental Readings\n\nOpenIntro Statistics slides\n\n\n\nVideos",
    "crumbs": [
      "Topics",
      "9 - Multiple and Logistic Regression"
    ]
  },
  {
    "objectID": "chapters/bayesian.html",
    "href": "chapters/bayesian.html",
    "title": "Bayesian Analysis",
    "section": "",
    "text": "Supplemental Readings\n\nChapter 17 of Learning Statistics with R (Navarro, version 0.6)\nFitting a Model by Maximum Likelihood (Collier, 2013).\nKruschke’s website for Doing Bayesian Data Analysis\nKruschke’s blog\nAndrew Gelman’s blog - Posts about bayesian statistics\n\n\n\nVideos\n\nRasmus Bååth’s Introduction to Bayesian Data Analysis Video Series\n\n\n\n\n\n\n\n\nJohn Kruschke’s Video Series\nBayesian Methods Interpret Data Better\n\n\nBayesian Estimation Supersedes the t Test\n\n\nPrecision is the goal",
    "crumbs": [
      "Topics",
      "Bayesian"
    ]
  }
]